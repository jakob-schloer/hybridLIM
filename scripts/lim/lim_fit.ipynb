{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Inverse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as ctp\n",
    "from importlib import reload\n",
    "\n",
    "from hyblim.model import lim\n",
    "from hyblim.data import preproc, eof \n",
    "from hyblim.utils import eval\n",
    "import hyblim.geoplot as gpl\n",
    "\n",
    "plt.style.use(\"../../paper.mplstyle\")\n",
    "\n",
    "# Parameters\n",
    "config = dict(vars=['ssta', 'ssha'], n_eof = [20, 10],\n",
    "              lim_type='cslim')\n",
    "config['datapaths'] = {}\n",
    "if 'ssta' in config['vars']:\n",
    "    config['datapaths']['ssta'] = \"../../data/cesm2-picontrol/b.e21.B1850.f09_g17.CMIP6-piControl.001.pop.h.ssta_lat-31_33_lon130_290_gr1.0.nc\"\n",
    "if 'ssha' in config['vars']:\n",
    "    config['datapaths']['ssha'] = \"../../data/cesm2-picontrol/b.e21.B1850.f09_g17.CMIP6-piControl.001.pop.h.ssha_lat-31_33_lon130_290_gr1.0.nc\"\n",
    "config['scorepath'] = \"../../models/lim/cslim_ssta-ssha/metrics\"\n",
    "config['plotpath'] = \"../../plots/lim/cslim_ssta-ssha/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_arr, normalizer = [], {}\n",
    "for var, path in config['datapaths'].items():\n",
    "    da = xr.open_dataset(path)[var]\n",
    "    # Normalize data \n",
    "    normalizer_var = preproc.Normalizer()\n",
    "    da = normalizer_var.fit_transform(da)\n",
    "    # Store normalizer as an attribute in the Dataarray for the inverse transformation\n",
    "    da.attrs = normalizer_var.to_dict()\n",
    "    da_arr.append(da)\n",
    "    normalizer[var] = normalizer_var\n",
    "\n",
    "ds = xr.merge(da_arr)\n",
    "\n",
    "# Apply land sea mask\n",
    "lsm = xr.open_dataset(\"../../data/land_sea_mask_common.nc\")['lsm']\n",
    "ds = ds.where(lsm!=1, other=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA\n",
    "eofa_lst = []\n",
    "for i, var in enumerate(ds.data_vars):\n",
    "    print(f\"Create EOF of {var}!\")\n",
    "    n_components = config['n_eof'][i] if isinstance(config['n_eof'], list) else config['n_eof'] \n",
    "    eofa = eof.EmpiricalOrthogonalFunctionAnalysis(n_components)\n",
    "    eofa.fit(\n",
    "        ds[var].isel(time=slice(None, int(0.8*len(ds['time']))))\n",
    "    )\n",
    "    eofa_lst.append(eofa)\n",
    "combined_eof = eof.CombinedEOF(eofa_lst, vars=list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in training and test data\n",
    "train_period = (0, int(0.8*len(ds['time'])))\n",
    "val_period = (int(0.8*len(ds['time'])), int(0.9*len(ds['time'])))\n",
    "test_period = ( int(0.9*len(ds['time'])), len(ds['time']) ) \n",
    "\n",
    "data = dict(\n",
    "    train = combined_eof.transform(ds.isel(time=slice(*train_period))),\n",
    "    val = combined_eof.transform(ds.isel(time=slice(*val_period))),\n",
    "    test = combined_eof.transform(ds.isel(time=slice(*test_period))),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lim)\n",
    "if config['lim_type'] == 'stlim':\n",
    "    model = lim.LIM(tau=1)\n",
    "    print(\"Fit ST-LIM\", flush=True)\n",
    "    model.fit(data['train'].data)\n",
    "    Q = model.noise_covariance()\n",
    "\n",
    "elif config['lim_type'] == 'cslim':\n",
    "    start_month = data['train'].time.dt.month[0].data\n",
    "    model = lim.CSLIM(tau=1)\n",
    "    print(\"Fit CS-LIM\", flush=True)\n",
    "    model.fit(data['train'].data.T, start_month, average_window=3)\n",
    "    Q = model.noise_covariance()\n",
    "else:\n",
    "    raise ValueError(\"lim_type not recognized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindcast mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc)\n",
    "def hindcast_mean(model, da: xr.DataArray, lag: int) -> xr.DataArray:\n",
    "    \"\"\"Hindcast LIM mean for a given lag.\n",
    "\n",
    "    Args:\n",
    "        model (_type_): LIM model. \n",
    "        da (xr.DataArray): Input dataarray. \n",
    "        lag (int, optional): Lag time.\n",
    "\n",
    "    Returns:\n",
    "        xr.DataArray: Forecast dataarray\n",
    "    \"\"\"\n",
    "    frcst_arr = []\n",
    "    for i, t in enumerate(da['time']):\n",
    "        month = t.dt.month.data\n",
    "        x_init = da.isel(time=i).data\n",
    "        x_frcst = model.forecast_mean(x_init, month, lag)\n",
    "        frcst_arr.append(x_frcst)\n",
    "\n",
    "    # Use initial times\n",
    "    da_frcst = xr.DataArray(data=frcst_arr, coords=dict(time=da['time'].data,\n",
    "                                                       eof=da['eof'].data))\n",
    "    da_frcst = da_frcst.assign_coords(coords=dict(lag=lag))\n",
    "    return da_frcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hindcast_arr, x_hindcast_arr = [], []\n",
    "lag_arr = [1, 3, 6 ,9, 12, 15, 18, 21, 24]\n",
    "\n",
    "for lag in lag_arr:\n",
    "    print(f\"Lag: {lag}\")\n",
    "    z = hindcast_mean(model, data['test'], lag=lag)\n",
    "    # Transform hindcast to grid space\n",
    "    x = eval.latent_frcst_to_grid(\n",
    "        z.values[:,np.newaxis,:], data['test'].time, combined_eof, normalizer, extended_eof=None\n",
    "    )\n",
    "    z_hindcast_arr.append(z)\n",
    "    x_hindcast_arr.append(x)\n",
    "z_hindcast_mean = xr.concat(z_hindcast_arr, dim=pd.Index(lag_arr, name='lag'))\n",
    "x_hindcast_mean = xr.concat(x_hindcast_arr, dim=pd.Index(lag_arr, name='lag'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_arr = np.arange(1, 25, 1)\n",
    "month_arr = np.arange(1,13,1)\n",
    "\n",
    "growth_rate, optimal_init_pc, optimal_evolved_pc = [], [], [] \n",
    "optimal_init_map, optimal_evolved_map = [], [] \n",
    "for i, month in enumerate(month_arr):\n",
    "    growth = np.zeros(len(lag_arr))\n",
    "    z_optimal_init, z_optimal_evolved = np.zeros((len(lag_arr), combined_eof.n_components)), np.zeros((len(lag_arr), combined_eof.n_components))\n",
    "    for i, lag in enumerate(lag_arr):\n",
    "        growth[i], z_optimal_init[i] = model.growth(month=month, lag=lag)\n",
    "        z_optimal_evolved[i] = np.real(model.forecast_mean(z_optimal_init[i].T, month=month, lag=lag))\n",
    "\n",
    "    growth_rate.append(xr.DataArray(data=growth, coords=dict(lag=lag_arr)))\n",
    "    optimal_init_pc.append(xr.DataArray(data=z_optimal_init, coords=dict(lag=lag_arr, eof=np.arange(combined_eof.n_components))))\n",
    "    optimal_evolved_pc.append(xr.DataArray(data=z_optimal_evolved, coords=dict(lag=lag_arr, eof=np.arange(combined_eof.n_components))))\n",
    "\n",
    "    # Optimal initial codition to grid space\n",
    "    x_opt_init = combined_eof.reconstruction(z_optimal_init, times=lag_arr)\n",
    "    x_opt_init = x_opt_init.rename_dims({'time':'lag'})\n",
    "    x_opt_evolved = combined_eof.reconstruction(z_optimal_evolved, times=lag_arr)\n",
    "    x_opt_evolved = x_opt_evolved.rename_dims({'time':'lag'})\n",
    "    optimal_init_map.append(x_opt_init)\n",
    "    optimal_evolved_map.append(x_opt_evolved)\n",
    "\n",
    "growth_rate = xr.concat(growth_rate, dim=pd.Index(month_arr, name='month'))\n",
    "optimal_init_pc = xr.concat(optimal_init_pc, dim=pd.Index(month_arr, name='month'))\n",
    "optimal_evolved_pc = xr.concat(optimal_evolved_pc, dim=pd.Index(month_arr, name='month'))\n",
    "optimal_init_map = xr.concat(optimal_init_map, dim=pd.Index(month_arr, name='month'))\n",
    "optimal_evolved_map = xr.concat(optimal_evolved_map, dim=pd.Index(month_arr, name='month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot growth\n",
    "month_label = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov','Dec']\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "for month in growth_rate['month'].data:\n",
    "    growth_rate.sel(month=month).plot(ax=ax, label=month_label[month-1])\n",
    "\n",
    "ax.set_xlabel(r\"$\\tau$\")\n",
    "ax.set_ylabel(r\"$log(\\gamma)$\")\n",
    "ax.set_title(rf\"Optimal growth ($\\tau_0$={model.tau_0})\")\n",
    "ax.grid('y')\n",
    "ax.legend(fontsize=11, bbox_to_anchor=(1,1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimal initial and evolved patterns\n",
    "reload(gpl)\n",
    "init_month = 5\n",
    "tau = 12\n",
    "pltargs = {'ssta': dict(vmin=-0.07, vmax=0.07, eps=0.01, cmap='RdBu_r', centercolor='white'),\n",
    "           'ssha': dict(vmin=-0.07, vmax=.07, eps=0.01, cmap='RdBu_r', centercolor='white')}\n",
    "\n",
    "nrow, ncol = len(optimal_init_map.data_vars), 2\n",
    "proj = ctp.crs.PlateCarree(central_longitude=180)\n",
    "fig, axs = plt.subplots(nrow, ncol, figsize=(7, nrow*2.5), subplot_kw={'projection': proj})\n",
    "\n",
    "for i, var in enumerate(optimal_init_map.data_vars):\n",
    "    ax = axs[i,0]\n",
    "    gpl.plot_map(optimal_init_map[var].sel(month=init_month, lag=tau), ax=ax, \n",
    "                 central_longitude=180, **pltargs[var])\n",
    "    ax.set_title(f\"Optimal initial (month={init_month}, lag={tau})\")\n",
    "\n",
    "    ax = axs[i,1]\n",
    "    gpl.plot_map(optimal_evolved_map[var].sel(month=init_month, lag=tau), ax=ax, \n",
    "                 central_longitude=180, **pltargs[var])\n",
    "    ax.set_title(f\"Evolved optimal {var} (month={init_month}, lag={tau})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "optimal_init_map.to_netcdf(config['scorepath'] + \"/optimal_init_map.nc\")\n",
    "optimal_evolved_map.to_netcdf(config['scorepath'] + \"/optimal_evolved_map.nc\")\n",
    "optimal_init_pc.to_netcdf(config['scorepath'] + \"/optimal_init_pc.nc\")\n",
    "optimal_evolved_pc.to_netcdf(config['scorepath'] + \"/optimal_evolved_pc.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project optimals on data (in pc-space) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_month = 12\n",
    "tau = 12\n",
    "dataset_key = 'test'\n",
    "\n",
    "# Get init and target data\n",
    "idx_val_init = np.where(data[dataset_key]['time.month'] == init_month)[0][:-1]\n",
    "z_val_init = data[dataset_key].isel(time=idx_val_init)\n",
    "z_val_target = data[dataset_key].isel(time=idx_val_init + tau)\n",
    "\n",
    "# Get optimal initial and evolved patterns in PC space\n",
    "opt_init = optimal_init_pc.sel(month=init_month, lag=tau)\n",
    "opt_evolved = optimal_evolved_pc.sel(month=init_month, lag=tau)\n",
    "\n",
    "proj_zdata_opt_init = opt_init.data @ z_val_init.data.T\n",
    "proj_zdata_opt_evolved = opt_evolved.data @ z_val_target.data.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hindcast in PC-space\n",
    "z_hat = model.forecast_mean(z_val_init.data.T, month=init_month, lag=tau)\n",
    "proj_zfrcst_opt_evolved = opt_evolved.data @ z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot(proj_zdata_opt_init, proj_zdata_opt_evolved, '.', label='Data')\n",
    "ax.plot(proj_zdata_opt_init, proj_zfrcst_opt_evolved, '.', label='LIM forecast mean')\n",
    "for ax in axs:\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Proj. on initial pattern\")\n",
    "    ax.set_ylabel(\"Proj. on evolved pattern\")\n",
    "\n",
    "plt.suptitle(f\"Optimal initial (month={init_month}, lag={tau})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def snr_square(model, da: xr.DataArray, lag: int) -> xr.DataArray:\n",
    "    \"\"\"Signal to noise ration over time squared for LIM.\n",
    "\n",
    "    Args:\n",
    "        model (CSLIM): CS-LIM model \n",
    "        da (xr.DataArray): \n",
    "        lag (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        xr.DataArray: _description_\n",
    "    \"\"\"\n",
    "    error_cov = np.zeros((12, len(da.eof), len(da.eof)))\n",
    "    for init_month in np.arange(1, 13):\n",
    "        target_month_idx = (init_month + lag -1) % 12 \n",
    "        error_cov[target_month_idx] = model.error_covariance(init_month, lag)\n",
    "\n",
    "    signal2noise_square = []\n",
    "    for i, t in enumerate(da['time']):\n",
    "        init_month = t.dt.month.data\n",
    "        target_month_idx = (init_month + lag -1) % 12 \n",
    "\n",
    "        # Hindcast\n",
    "        z0 = da.isel(time=i).data\n",
    "        z_frcst = model.forecast_mean(z0, init_month, lag)\n",
    "\n",
    "        # Signal covariance f(tau, month, t) = x(tau, t) x(tau, t)^T\n",
    "        signal_cov = z_frcst[:,np.newaxis] @ z_frcst[:, np.newaxis].T\n",
    "        error_cov_month = error_cov[target_month_idx]\n",
    "\n",
    "        signal2noise_square.append(np.trace(signal_cov) / np.trace(error_cov_month))\n",
    "\n",
    "    target_time = da['time'].data[lag:]\n",
    "    signal2noise_square = xr.DataArray(\n",
    "        signal2noise_square[:-lag], \n",
    "        coords={'time': target_time}\n",
    "    )\n",
    "    return signal2noise_square\n",
    "\n",
    "\n",
    "dataset_key = 'test'\n",
    "da = data['test']\n",
    "lag_arr = [1, 3, 6, 9, 12, 15, 18, 24]\n",
    "snr_arr = []\n",
    "for lag in tqdm(lag_arr):\n",
    "    snr_arr.append(snr_square(model, da, lag))\n",
    "\n",
    "snr_square = xr.concat(snr_arr, dim=pd.Index(lag_arr, name='lag'),\n",
    "                       compat='equals', join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SNR^2 and theroretical correlation coefficient\n",
    "lag = 6\n",
    "correlation_coeff_theory = snr_square.sel(lag=lag) / np.sqrt( (1 + snr_square.sel(lag=lag)) *snr_square.sel(lag=lag) )\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "snr_square.sel(lag=lag).plot(ax=axs[0], label=r'$SNR^2$')\n",
    "axs[0].set_ylabel(r'$SNR^2$')  \n",
    "correlation_coeff_theory.plot(ax=axs[1], label=r'$\\rho_\\infty$')\n",
    "axs[1].set_ylabel(r'$\\rho_\\infty$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average SNR^2 over time for months and lad time\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "snr_month = snr_square.groupby('time.month').mean('time')\n",
    "axs[0].plot(snr_month.month, snr_month.sel(lag=lag).data, 'o-', label=rf'$\\tau={lag}$')\n",
    "axs[0].set_xlabel(\"target month\")\n",
    "axs[0].set_ylabel(r\"$SNR^2$\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(snr_square['lag'], snr_square.mean(dim='time'), 'o-')\n",
    "axs[1].set_xlabel(r\"$\\tau$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SNR^2 to file\n",
    "snr_square.to_dataset(name='snr2').to_netcdf(config['scorepath'] + f\"/snr2_{dataset_key}_data.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nyquist mode as function of number of EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "reload(eof)\n",
    "n_comp_arr = np.arange(2,25,2)\n",
    "G_max = []\n",
    "G_min = []\n",
    "for n_components in n_comp_arr:\n",
    "    print(f\"Num of eofs: {n_components}\")\n",
    "    pca_lst = []\n",
    "    for i, var in enumerate(ds.data_vars):\n",
    "        print(f\"Create EOF of {var}!\")\n",
    "        n_components = n_components // (2**i)\n",
    "        eofa = eof.EmpiricalOrthogonalFunctionAnalysis(n_components=n_components)\n",
    "        eofa.fit(ds[var])\n",
    "        pca_lst.append(eofa)\n",
    "    combined_eof = eof.CombinedEOF(pca_lst, vars=list(ds.data_vars))\n",
    "\n",
    "    # Split in training and test data\n",
    "    train_period = (0, int(0.8*len(ds['time'])))\n",
    "    data_train = combined_eof.transform(ds.isel(time=slice(*train_period)))\n",
    "\n",
    "    # Fit CSLIM\n",
    "    cslim = lim.CSLIM(tau=1)\n",
    "    start_month = data_train.time.dt.month[0].data\n",
    "    cslim.fit(data_train.data.T, start_month, average_window=5)\n",
    "\n",
    "    # Check for nyquist mode of G_T = G12 ... G1\n",
    "    G_arr = []\n",
    "    for i in range(cslim.G.shape[0]):\n",
    "        idx = i % 12\n",
    "        G_arr.append(cslim.G[idx])\n",
    "    G_T = functools.reduce(np.dot, G_arr[::-1])\n",
    "\n",
    "    # Check eigenvalues are between 0 and 1\n",
    "    w, U, V = lim.matrix_decomposition(G_T)\n",
    "    G_max.append(np.max(w))\n",
    "    G_min.append(np.min(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "G_min = np.array(G_min)\n",
    "G_max = np.array(G_max)\n",
    "\n",
    "ax.bar(n_comp_arr, height=G_max- G_min, bottom=G_min,\n",
    "           label=r'[min($\\lambda_{G}$), max($\\lambda_{G}$)]')\n",
    "\n",
    "ax.axhline(0, color='k')\n",
    "#ax.axhline(1, color='k')\n",
    "# ax.set_xticks(n_comp_arr)\n",
    "# ax.set_ylim(-.1, .6)\n",
    "ax.set_xlabel('# of EOF')\n",
    "ax.set_ylabel(r\"$\\lambda_G$\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('vaeenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd1f84a93514db3fb3689a6c2d4c248cfb632ba5f8c260d8b9cf936021326503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
